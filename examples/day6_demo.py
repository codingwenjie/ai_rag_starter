"""Day6 å‘é‡æ•°æ®åº“å­¦ä¹ ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨embeddingã€å‘é‡å­˜å‚¨å’Œé—®ç­”ç³»ç»Ÿ
"""

import asyncio
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_core.documents import Document
from services.embedding_service import EmbeddingService
from services.vector_store_service import VectorStoreService
from services.qa_system import QASystem


async def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®Œæ•´çš„å‘é‡æ•°æ®åº“ä½¿ç”¨æµç¨‹"""
    print("ğŸš€ Day6 å‘é‡æ•°æ®åº“å­¦ä¹ ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. åˆå§‹åŒ–æœåŠ¡
    print("\nğŸ“ æ­¥éª¤1: åˆå§‹åŒ–æœåŠ¡")
    embedding_service = EmbeddingService()
    vector_store_service = VectorStoreService(embedding_service)
    qa_system = QASystem(embedding_service, vector_store_service)
    
    # 2. å‡†å¤‡æµ‹è¯•æ–‡æ¡£
    print("\nğŸ“š æ­¥éª¤2: å‡†å¤‡æµ‹è¯•æ–‡æ¡£")
    documents = [
        Document(
            page_content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»åã€‚å®ƒå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸã€‚",
            metadata={"source": "python_intro.txt", "category": "ç¼–ç¨‹è¯­è¨€"}
        ),
        Document(
            page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰ã€‚",
            metadata={"source": "ml_basics.txt", "category": "æœºå™¨å­¦ä¹ "}
        ),
        Document(
            page_content="å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨è®¾è®¡ç”¨æ¥å­˜å‚¨å’ŒæŸ¥è¯¢é«˜ç»´å‘é‡æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿã€‚å®ƒä»¬åœ¨ç›¸ä¼¼æ€§æœç´¢ã€æ¨èç³»ç»Ÿå’Œè¯­ä¹‰æœç´¢ç­‰åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚",
            metadata={"source": "vector_db.txt", "category": "æ•°æ®åº“"}
        ),
        Document(
            page_content="FAISSï¼ˆFacebook AI Similarity Searchï¼‰æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆç›¸ä¼¼æ€§æœç´¢å’Œå¯†é›†å‘é‡èšç±»çš„åº“ã€‚å®ƒå¯ä»¥å¤„ç†ä»»æ„å¤§å°çš„å‘é‡é›†åˆï¼Œç”šè‡³æ˜¯å†…å­˜æ— æ³•å®¹çº³çš„å‘é‡é›†åˆã€‚",
            metadata={"source": "faiss_intro.txt", "category": "å·¥å…·åº“"}
        ),
        Document(
            page_content="LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒæä¾›äº†æ¨¡å—åŒ–çš„ç»„ä»¶ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿè½»æ¾æ„å»ºå¤æ‚çš„AIåº”ç”¨ã€‚",
            metadata={"source": "langchain_intro.txt", "category": "AIæ¡†æ¶"}
        )
    ]
    
    print(f"å‡†å¤‡äº† {len(documents)} ä¸ªæµ‹è¯•æ–‡æ¡£")
    
    # 3. æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
    print("\nğŸ”„ æ­¥éª¤3: å‘é‡åŒ–å¹¶å­˜å‚¨æ–‡æ¡£")
    doc_ids = await vector_store_service.add_documents(documents)
    print(f"æˆåŠŸæ·»åŠ æ–‡æ¡£ï¼ŒID: {doc_ids}")
    
    # 4. æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢
    print("\nğŸ” æ­¥éª¤4: æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢")
    search_queries = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
        "å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for query in search_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = await vector_store_service.semantic_search(query, k=2)
        
        for result in results:
            print(f"  ğŸ“„ æ’å {result['rank']}: {result['content'][:60]}...")
            print(f"     ç›¸ä¼¼åº¦: {result['similarity_score']:.4f} | ç›¸å…³æ€§: {result['relevance']}")
    
    # 5. æµ‹è¯•é—®ç­”ç³»ç»Ÿ
    print("\nğŸ’¬ æ­¥éª¤5: æµ‹è¯•é—®ç­”ç³»ç»Ÿ")
    qa_questions = [
        "Pythonæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "FAISSæ˜¯ä»€ä¹ˆï¼Ÿ",
        "LangChainçš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for question in qa_questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        answer = await qa_system.ask(question)
        
        print(f"ğŸ¤– ç­”æ¡ˆ: {answer['answer']}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {answer['confidence']}")
        
        if answer['sources']:
            print("ğŸ“š å‚è€ƒæ¥æº:")
            for i, source in enumerate(answer['sources'][:2], 1):
                print(f"  {i}. {source['content'][:50]}...")
    
    # 6. ä¿å­˜å‘é‡ç´¢å¼•
    print("\nğŸ’¾ æ­¥éª¤6: ä¿å­˜å‘é‡ç´¢å¼•")
    success = vector_store_service.save_index()
    if success:
        print("âœ… å‘é‡ç´¢å¼•ä¿å­˜æˆåŠŸ")
    else:
        print("âŒ å‘é‡ç´¢å¼•ä¿å­˜å¤±è´¥")
    
    # 7. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ­¥éª¤7: ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
    stats = vector_store_service.get_stats()
    print(f"æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
    print(f"å‘é‡ç»´åº¦: {stats.get('vector_dimension', 'N/A')}")
    print(f"æœ€åæ›´æ–°: {stats.get('last_updated', 'N/A')}")
    
    print("\nğŸ‰ Day6 å­¦ä¹ ç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ“– å­¦ä¹ è¦ç‚¹æ€»ç»“:")
    print("1. âœ… æŒæ¡äº†æ–‡æœ¬å‘é‡åŒ–çš„åŸºæœ¬åŸç†")
    print("2. âœ… å­¦ä¼šäº†ä½¿ç”¨FAISSè¿›è¡Œå‘é‡å­˜å‚¨å’Œæ£€ç´¢")
    print("3. âœ… ç†è§£äº†ç›¸ä¼¼åº¦æœç´¢çš„å·¥ä½œæœºåˆ¶")
    print("4. âœ… æ„å»ºäº†åŸºäºå‘é‡æ£€ç´¢çš„é—®ç­”ç³»ç»Ÿ")
    print("5. âœ… å­¦ä¼šäº†å‘é‡ç´¢å¼•çš„æŒä¹…åŒ–å­˜å‚¨")


if __name__ == "__main__":
    # ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·å…ˆè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        exit(1)
    
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())