机器学习算法详解

一、线性回归

线性回归是最基础的机器学习算法之一，用于预测连续数值。它假设因变量与自变量之间存在线性关系。

数学公式：y = wx + b
其中：
- y是预测值
- x是输入特征
- w是权重（斜率）
- b是偏置（截距）

损失函数：均方误差（MSE）
MSE = (1/n) * Σ(yi - ŷi)²

优化方法：
1. 梯度下降法
2. 正规方程法
3. 随机梯度下降

应用场景：
- 房价预测
- 销售预测
- 股价分析

二、逻辑回归

逻辑回归用于二分类问题，通过Sigmoid函数将线性回归的输出映射到0-1之间。

Sigmoid函数：σ(z) = 1 / (1 + e^(-z))

损失函数：交叉熵损失
Loss = -[y*log(p) + (1-y)*log(1-p)]

优点：
- 计算简单
- 不需要特征缩放
- 输出概率值
- 不容易过拟合

缺点：
- 假设线性关系
- 对异常值敏感
- 需要大样本量

三、决策树

决策树是一种基于树结构的分类和回归算法，通过一系列if-else条件来做决策。

分裂准则：
1. 信息增益（ID3算法）
2. 信息增益率（C4.5算法）
3. 基尼不纯度（CART算法）

信息熵：H(S) = -Σ p(i) * log2(p(i))

基尼不纯度：Gini(S) = 1 - Σ p(i)²

剪枝方法：
- 预剪枝：在构建过程中提前停止
- 后剪枝：构建完成后删除部分节点

优点：
- 易于理解和解释
- 不需要数据预处理
- 能处理数值和类别特征
- 能处理缺失值

缺点：
- 容易过拟合
- 对噪声敏感
- 不稳定

四、随机森林

随机森林是集成学习方法，通过构建多个决策树并投票来提高预测准确性。

核心思想：
1. Bootstrap采样：有放回地抽取训练样本
2. 随机特征选择：每次分裂时随机选择特征子集
3. 多数投票：分类问题取多数票，回归问题取平均值

参数设置：
- n_estimators：树的数量
- max_features：每次分裂考虑的特征数
- max_depth：树的最大深度
- min_samples_split：分裂所需的最小样本数

优点：
- 准确率高
- 能处理大数据集
- 能评估特征重要性
- 对缺失值不敏感
- 不容易过拟合

缺点：
- 模型复杂，难以解释
- 内存消耗大
- 对噪声敏感

五、支持向量机（SVM）

SVM通过寻找最优超平面来分离不同类别的数据点。

核心概念：
- 支持向量：距离超平面最近的数据点
- 间隔：支持向量到超平面的距离
- 核函数：将数据映射到高维空间

常用核函数：
1. 线性核：K(x,y) = x·y
2. 多项式核：K(x,y) = (x·y + c)^d
3. RBF核：K(x,y) = exp(-γ||x-y||²)
4. Sigmoid核：K(x,y) = tanh(αx·y + c)

参数调优：
- C：正则化参数，控制对误分类的惩罚
- γ：RBF核的参数，控制单个样本的影响范围

优点：
- 在高维空间有效
- 内存使用高效
- 适用于特征数大于样本数的情况

缺点：
- 训练时间长
- 对特征缩放敏感
- 不提供概率估计

六、K-means聚类

K-means是最常用的无监督学习算法，将数据分为K个簇。

算法步骤：
1. 随机初始化K个聚类中心
2. 将每个数据点分配到最近的聚类中心
3. 更新聚类中心为簇内所有点的均值
4. 重复步骤2-3直到收敛

距离度量：
- 欧几里得距离：d = √Σ(xi - yi)²
- 曼哈顿距离：d = Σ|xi - yi|
- 余弦距离：d = 1 - (x·y)/(||x||·||y||)

评估指标：
- 轮廓系数（Silhouette Score）
- 肘部法则（Elbow Method）
- 卡林斯基-哈拉巴斯指数

优点：
- 简单易实现
- 计算效率高
- 适用于球形簇

缺点：
- 需要预先指定K值
- 对初始化敏感
- 假设簇是球形的
- 对异常值敏感

七、朴素贝叶斯

朴素贝叶斯基于贝叶斯定理和特征条件独立假设。

贝叶斯定理：P(A|B) = P(B|A) * P(A) / P(B)

分类公式：
P(类别|特征) = P(特征|类别) * P(类别) / P(特征)

常见变种：
1. 高斯朴素贝叶斯：适用于连续特征
2. 多项式朴素贝叶斯：适用于离散特征
3. 伯努利朴素贝叶斯：适用于二元特征

优点：
- 训练和预测速度快
- 对小样本表现良好
- 对缺失数据不敏感
- 适用于多分类问题

缺点：
- 强独立性假设
- 对特征相关性敏感
- 需要平滑处理零概率

八、神经网络

神经网络模拟人脑神经元的工作方式，通过多层感知器进行学习。

基本组成：
- 输入层：接收原始数据
- 隐藏层：进行特征变换
- 输出层：产生最终结果

激活函数：
1. Sigmoid：σ(x) = 1/(1+e^(-x))
2. ReLU：f(x) = max(0,x)
3. Tanh：tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
4. Softmax：σ(xi) = e^xi / Σe^xj

反向传播算法：
1. 前向传播计算输出
2. 计算损失函数
3. 反向传播计算梯度
4. 更新权重和偏置

优化算法：
- SGD：随机梯度下降
- Adam：自适应矩估计
- RMSprop：均方根传播
- AdaGrad：自适应梯度

正则化技术：
- L1正则化：权重绝对值之和
- L2正则化：权重平方和
- Dropout：随机丢弃神经元
- 批量归一化：标准化输入

优点：
- 强大的非线性建模能力
- 自动特征学习
- 适用于复杂问题

缺点：
- 需要大量数据
- 训练时间长
- 容易过拟合
- 黑盒模型，难以解释

总结：

选择合适的机器学习算法需要考虑多个因素：
1. 问题类型：分类、回归、聚类
2. 数据规模：样本数量和特征维度
3. 数据质量：噪声、缺失值、异常值
4. 性能要求：准确率、速度、内存
5. 可解释性：是否需要理解模型决策过程

实际应用中，通常需要尝试多种算法并进行交叉验证来选择最佳模型。