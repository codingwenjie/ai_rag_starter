<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>深度学习入门指南</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
        h1 { color: #2c3e50; }
        h2 { color: #34495e; }
        h3 { color: #7f8c8d; }
        code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 3px; }
        .highlight { background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107; }
    </style>
</head>
<body>
    <h1>深度学习入门指南</h1>
    
    <h2>1. 什么是深度学习</h2>
    <p>深度学习是机器学习的一个子领域，它基于人工神经网络的表示学习。深度学习的"深度"指的是网络中的层数，通常包含多个隐藏层。</p>
    
    <div class="highlight">
        <strong>核心概念：</strong>深度学习通过多层神经网络自动学习数据的层次化特征表示，从而解决复杂的模式识别问题。
    </div>
    
    <h3>1.1 深度学习的优势</h3>
    <ul>
        <li><strong>自动特征提取：</strong>无需手动设计特征，网络自动学习有用的特征</li>
        <li><strong>端到端学习：</strong>从原始输入直接学习到最终输出</li>
        <li><strong>强大的表示能力：</strong>能够学习复杂的非线性映射关系</li>
        <li><strong>可扩展性：</strong>随着数据量增加，性能通常会提升</li>
    </ul>
    
    <h2>2. 神经网络基础</h2>
    
    <h3>2.1 感知器模型</h3>
    <p>感知器是最简单的神经网络单元，包含以下组件：</p>
    <ul>
        <li><strong>输入：</strong>x₁, x₂, ..., xₙ</li>
        <li><strong>权重：</strong>w₁, w₂, ..., wₙ</li>
        <li><strong>偏置：</strong>b</li>
        <li><strong>激活函数：</strong>f(z)</li>
    </ul>
    
    <p>数学表达式：<code>y = f(∑wᵢxᵢ + b)</code></p>
    
    <h3>2.2 多层感知器（MLP）</h3>
    <p>多层感知器由多个层组成：</p>
    <ol>
        <li><strong>输入层：</strong>接收原始数据</li>
        <li><strong>隐藏层：</strong>进行特征变换（可以有多个）</li>
        <li><strong>输出层：</strong>产生最终预测结果</li>
    </ol>
    
    <h2>3. 激活函数</h2>
    
    <h3>3.1 常用激活函数</h3>
    <table border="1" style="border-collapse: collapse; width: 100%;">
        <tr>
            <th>函数名</th>
            <th>公式</th>
            <th>特点</th>
            <th>应用场景</th>
        </tr>
        <tr>
            <td>Sigmoid</td>
            <td>σ(x) = 1/(1+e⁻ˣ)</td>
            <td>输出范围(0,1)，容易饱和</td>
            <td>二分类输出层</td>
        </tr>
        <tr>
            <td>Tanh</td>
            <td>tanh(x) = (eˣ-e⁻ˣ)/(eˣ+e⁻ˣ)</td>
            <td>输出范围(-1,1)，零中心</td>
            <td>隐藏层</td>
        </tr>
        <tr>
            <td>ReLU</td>
            <td>f(x) = max(0,x)</td>
            <td>计算简单，缓解梯度消失</td>
            <td>隐藏层（最常用）</td>
        </tr>
        <tr>
            <td>Leaky ReLU</td>
            <td>f(x) = max(αx,x)</td>
            <td>解决ReLU死神经元问题</td>
            <td>隐藏层</td>
        </tr>
        <tr>
            <td>Softmax</td>
            <td>σ(xᵢ) = eˣⁱ/∑eˣʲ</td>
            <td>输出概率分布</td>
            <td>多分类输出层</td>
        </tr>
    </table>
    
    <h2>4. 损失函数</h2>
    
    <h3>4.1 回归问题</h3>
    <ul>
        <li><strong>均方误差（MSE）：</strong><code>L = (1/n)∑(yᵢ - ŷᵢ)²</code></li>
        <li><strong>平均绝对误差（MAE）：</strong><code>L = (1/n)∑|yᵢ - ŷᵢ|</code></li>
        <li><strong>Huber损失：</strong>结合MSE和MAE的优点</li>
    </ul>
    
    <h3>4.2 分类问题</h3>
    <ul>
        <li><strong>交叉熵损失：</strong><code>L = -∑yᵢlog(ŷᵢ)</code></li>
        <li><strong>二元交叉熵：</strong><code>L = -[ylog(ŷ) + (1-y)log(1-ŷ)]</code></li>
        <li><strong>Focal Loss：</strong>解决类别不平衡问题</li>
    </ul>
    
    <h2>5. 优化算法</h2>
    
    <h3>5.1 梯度下降法</h3>
    <p>基本思想：沿着梯度的反方向更新参数</p>
    <code>θ = θ - α∇J(θ)</code>
    
    <h3>5.2 改进的优化算法</h3>
    <ul>
        <li><strong>SGD with Momentum：</strong>加入动量项，加速收敛</li>
        <li><strong>AdaGrad：</strong>自适应学习率</li>
        <li><strong>RMSprop：</strong>解决AdaGrad学习率递减问题</li>
        <li><strong>Adam：</strong>结合Momentum和RMSprop的优点</li>
        <li><strong>AdamW：</strong>改进的权重衰减方法</li>
    </ul>
    
    <h2>6. 正则化技术</h2>
    
    <h3>6.1 权重正则化</h3>
    <ul>
        <li><strong>L1正则化：</strong>L = L₀ + λ∑|wᵢ|（产生稀疏性）</li>
        <li><strong>L2正则化：</strong>L = L₀ + λ∑wᵢ²（权重衰减）</li>
    </ul>
    
    <h3>6.2 Dropout</h3>
    <p>训练时随机将一些神经元的输出设为0，防止过拟合</p>
    <ul>
        <li>训练时：随机丢弃p比例的神经元</li>
        <li>测试时：使用所有神经元，输出乘以(1-p)</li>
    </ul>
    
    <h3>6.3 批量归一化（Batch Normalization）</h3>
    <p>对每个mini-batch的输入进行标准化：</p>
    <code>BN(x) = γ((x-μ)/σ) + β</code>
    
    <div class="highlight">
        <strong>优点：</strong>加速训练、提高稳定性、具有正则化效果
    </div>
    
    <h2>7. 卷积神经网络（CNN）</h2>
    
    <h3>7.1 卷积层</h3>
    <p>卷积操作通过滤波器（kernel）提取局部特征：</p>
    <ul>
        <li><strong>卷积核大小：</strong>通常为3×3或5×5</li>
        <li><strong>步长（stride）：</strong>卷积核移动的步长</li>
        <li><strong>填充（padding）：</strong>在输入边缘添加0</li>
    </ul>
    
    <h3>7.2 池化层</h3>
    <p>降低特征图的空间维度：</p>
    <ul>
        <li><strong>最大池化：</strong>取窗口内的最大值</li>
        <li><strong>平均池化：</strong>取窗口内的平均值</li>
        <li><strong>全局池化：</strong>对整个特征图进行池化</li>
    </ul>
    
    <h3>7.3 经典CNN架构</h3>
    <ul>
        <li><strong>LeNet-5：</strong>最早的CNN架构</li>
        <li><strong>AlexNet：</strong>深度学习复兴的标志</li>
        <li><strong>VGG：</strong>使用小卷积核的深层网络</li>
        <li><strong>ResNet：</strong>引入残差连接解决梯度消失</li>
        <li><strong>Inception：</strong>多尺度特征提取</li>
    </ul>
    
    <h2>8. 循环神经网络（RNN）</h2>
    
    <h3>8.1 基本RNN</h3>
    <p>RNN能够处理序列数据，具有记忆能力：</p>
    <code>hₜ = tanh(Wₓₕxₜ + Wₕₕhₜ₋₁ + bₕ)</code>
    
    <h3>8.2 RNN的问题</h3>
    <ul>
        <li><strong>梯度消失：</strong>长序列训练困难</li>
        <li><strong>梯度爆炸：</strong>梯度值过大导致不稳定</li>
    </ul>
    
    <h3>8.3 改进的RNN架构</h3>
    <ul>
        <li><strong>LSTM：</strong>长短期记忆网络，解决梯度消失问题</li>
        <li><strong>GRU：</strong>门控循环单元，LSTM的简化版本</li>
        <li><strong>双向RNN：</strong>同时考虑前向和后向信息</li>
    </ul>
    
    <h2>9. Transformer架构</h2>
    
    <h3>9.1 注意力机制</h3>
    <p>注意力机制允许模型关注输入的不同部分：</p>
    <code>Attention(Q,K,V) = softmax(QKᵀ/√dₖ)V</code>
    
    <h3>9.2 自注意力（Self-Attention）</h3>
    <p>输入序列的每个位置都可以关注序列中的所有位置</p>
    
    <h3>9.3 多头注意力</h3>
    <p>并行计算多个注意力头，捕获不同类型的依赖关系</p>
    
    <h3>9.4 位置编码</h3>
    <p>由于Transformer没有循环结构，需要添加位置信息</p>
    
    <h2>10. 深度学习应用领域</h2>
    
    <h3>10.1 计算机视觉</h3>
    <ul>
        <li><strong>图像分类：</strong>识别图像中的对象类别</li>
        <li><strong>目标检测：</strong>定位和识别图像中的多个对象</li>
        <li><strong>图像分割：</strong>像素级别的分类</li>
        <li><strong>人脸识别：</strong>身份验证和识别</li>
        <li><strong>图像生成：</strong>GAN、VAE等生成模型</li>
    </ul>
    
    <h3>10.2 自然语言处理</h3>
    <ul>
        <li><strong>机器翻译：</strong>自动翻译不同语言</li>
        <li><strong>文本分类：</strong>情感分析、主题分类</li>
        <li><strong>问答系统：</strong>自动回答用户问题</li>
        <li><strong>文本生成：</strong>自动写作、对话系统</li>
        <li><strong>语音识别：</strong>语音转文本</li>
    </ul>
    
    <h3>10.3 其他应用</h3>
    <ul>
        <li><strong>推荐系统：</strong>个性化推荐</li>
        <li><strong>游戏AI：</strong>强化学习在游戏中的应用</li>
        <li><strong>自动驾驶：</strong>环境感知和决策</li>
        <li><strong>医疗诊断：</strong>医学图像分析</li>
        <li><strong>金融风控：</strong>欺诈检测和风险评估</li>
    </ul>
    
    <h2>11. 深度学习框架</h2>
    
    <h3>11.1 主流框架对比</h3>
    <table border="1" style="border-collapse: collapse; width: 100%;">
        <tr>
            <th>框架</th>
            <th>开发者</th>
            <th>特点</th>
            <th>适用场景</th>
        </tr>
        <tr>
            <td>TensorFlow</td>
            <td>Google</td>
            <td>生产级部署、TensorBoard可视化</td>
            <td>工业应用、大规模部署</td>
        </tr>
        <tr>
            <td>PyTorch</td>
            <td>Facebook</td>
            <td>动态图、易于调试</td>
            <td>研究、快速原型</td>
        </tr>
        <tr>
            <td>Keras</td>
            <td>François Chollet</td>
            <td>高级API、易于使用</td>
            <td>快速开发、教学</td>
        </tr>
        <tr>
            <td>JAX</td>
            <td>Google</td>
            <td>函数式编程、自动微分</td>
            <td>科学计算、研究</td>
        </tr>
    </table>
    
    <h2>12. 实践建议</h2>
    
    <h3>12.1 数据准备</h3>
    <ul>
        <li><strong>数据质量：</strong>确保数据的准确性和完整性</li>
        <li><strong>数据增强：</strong>通过变换增加数据多样性</li>
        <li><strong>数据标准化：</strong>将数据缩放到合适的范围</li>
        <li><strong>数据分割：</strong>合理划分训练集、验证集和测试集</li>
    </ul>
    
    <h3>12.2 模型设计</h3>
    <ul>
        <li><strong>从简单开始：</strong>先尝试简单模型，再逐步复杂化</li>
        <li><strong>合适的架构：</strong>根据问题选择合适的网络结构</li>
        <li><strong>超参数调优：</strong>系统性地调整学习率、批大小等</li>
        <li><strong>正则化：</strong>防止过拟合</li>
    </ul>
    
    <h3>12.3 训练技巧</h3>
    <ul>
        <li><strong>学习率调度：</strong>动态调整学习率</li>
        <li><strong>早停法：</strong>防止过拟合</li>
        <li><strong>模型检查点：</strong>保存最佳模型</li>
        <li><strong>梯度裁剪：</strong>防止梯度爆炸</li>
    </ul>
    
    <div class="highlight">
        <strong>重要提示：</strong>深度学习需要大量的实践和实验。理论知识很重要，但动手实践更加关键。建议从简单的项目开始，逐步提高复杂度。
    </div>
    
    <h2>13. 学习资源推荐</h2>
    
    <h3>13.1 在线课程</h3>
    <ul>
        <li>Andrew Ng的深度学习专项课程（Coursera）</li>
        <li>Fast.ai实用深度学习课程</li>
        <li>CS231n：卷积神经网络（Stanford）</li>
        <li>CS224n：自然语言处理（Stanford）</li>
    </ul>
    
    <h3>13.2 经典书籍</h3>
    <ul>
        <li>《深度学习》- Ian Goodfellow等</li>
        <li>《动手学深度学习》- 李沐等</li>
        <li>《Python深度学习》- François Chollet</li>
        <li>《深度学习入门》- 斋藤康毅</li>
    </ul>
    
    <h3>13.3 实践平台</h3>
    <ul>
        <li>Kaggle：数据科学竞赛平台</li>
        <li>Google Colab：免费GPU环境</li>
        <li>Papers with Code：论文和代码</li>
        <li>GitHub：开源项目</li>
    </ul>
    
    <h2>总结</h2>
    <p>深度学习是一个快速发展的领域，需要扎实的数学基础和大量的实践经验。从基础的神经网络开始，逐步学习CNN、RNN、Transformer等架构，并在实际项目中应用所学知识。保持对新技术的关注，持续学习和实践是成功的关键。</p>
    
</body>
</html>